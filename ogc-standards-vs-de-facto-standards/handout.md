# Best practices for discovery, processing and analysis of distributed spatial data - OGC standards vs. de facto standards

Author: @niiiiikd

## Introduction

In this summary I want to explore the potential usability of de facto standards in geospatial analysis technology in comparison to existing OGC (Open Geospatial Consortium) standards.

## OGC Standarts

OGC standards can be divided into three types, the first being the „Abstract specification topics“, which describe the more conceptual foundation on which the actual standards are built. The next type of OGC standards is the „Member Developed Standard“, which as the name implies can be any standard developed by the OGC technical committee. Finally, we have the „Community Standards“. These are standards which were developed external to OGC but which were then later adapted by the organization. This is important to understand, since it means that not any OGC standard must have been developed by OGC. The organization itself claims that the various standards serve the purpose of making location data „Fair - Findable, Accessible, Interoperable and Reusable“.  

One example of a OGC standard is the Well Known Text (WKT) representation of Coordinate Reference Systems, which enables users to describe any Reference System following the guidelines of ISO-19111:2007 and ISO-19111:2009. In addition, this standard was updated to allow users to also describe coordinate operations. Further, the geoTIFF is actually a OGC standard, too. It was developed to allow interoperable implementations of georeferenced TIFF images, which are very commonly used for satellite imagery in geospatial technology. Looking at the challenges the Internet of Things (IoT) may bring, the OGC also developed the SensorThings Application Programming Interface, to allow for seamless communication between many different types of sensors and their links over the web, which could aid the implementation of smart cities for example. There are many other standards developed or adapted by the organization. An overview of future OGC standards is available on the „OGC Standards Roadmap“. Here standards currently in the research or development phase can be looked at in the form of a useful table, which also gives a good glance at the production process behind any OGC standard. 

## De Facto Standarts

The standards described by the organization are in use in a vast array of applications and implementations. This leads to better interoperability, since software engineers can create compatible services without even knowing each other's work by using the official standards. But if it is so useful, how come there are several technologies in place, which became quasi or de-facto standards, not by means of official organizations but through the sheer amount of usage they experienced?

One example for a de facto standard like this is STAC, or „Spatio Temporal Asset Catalog“, a service which allows users to query for satellite imagery through metadata. It is implemented in many solutions, for example in the Google Earth Engine, Earth on Demand and Sentinel Hub. Another example for a de facto standard is GDAL. GDAL is a translation library for both geospatial raster and vector data, and is also not a official OGC standard, despite its heavy usage in existing technologies. „GDAL“ is short for „Geospatial Data Abstraction Library“. Famous GIS applications such as ArcGIS, Grass GIS and QGIS implement the library.

## Conclusion

One advantage of official standards is the thorough documentation. This is useful when questions or issues arise during implementation. By using a de facto standard you can not expect the same level of documentation as you may be used to by OGC standards. Another advantage of using OGC standards is that you can assure maximum interoperability by implementing those instead of de facto standards. If your implementation relies heavily on interoperability I would suggest using an official OGC standard because of this. A benefit of using a de facto standard is that you may get access to the best, latest technologies, since an organisation like OGC may be late in adapting such technologies due to their lengthy review and discussion process. Thus, if you find yourself in a situation where developers left and right of you are implementing their application using de facto standards and are seeing great results in doing so, why bother with an old standard just for an increase in interoperability, and risk providing a worse or even slower application in the process? So if interoperability is not a must have for your personal needs, you should seriously assess the viability of implementation with a de facto standard, if it provides obvious benefits to you or your company. It is also good to know that you can always fall back on the OGC standards if the de facto standards end up causing you trouble. This may cost you precious development time, but this risk can also be mitigated by creating a sort of contingency plan in case things do not work out as expected or support for the de facto standard drops, which is another unfortunate thing that could happen. Of course you could also find yourself in a situation where a de facto standard provides obvious benefits to you, such as new features or possibilities which simply did not exist before. In this case you obviously need to implement the de facto standard, since there are no alternatives to choose from other than implementing the functionality yourself, which might turn out to be quite burdensome. In this case, you could keep your eyes open for upcoming OGC standards and implement these when available. In the end, it remains a very personal decision which could depend on the type of application you are trying to create. If your application needs to ensure interoperability first and foremost, I would suggest using the OGC standards if available. Otherwise exploring some of the latest technologies which just might not have been around long enough yet to receive the official stamp of approval can be a worthy option.
